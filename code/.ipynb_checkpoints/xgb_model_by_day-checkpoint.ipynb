{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "# 671112131516\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataofweek/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义评价函数\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取base_feature\n",
    "def get_base_feature(df):\n",
    "    df['time'] = df.START_DATE.apply(lambda x:x[:19])\n",
    "    df['time'] = pd.to_datetime(df['time'],format='%Y-%m-%d-%H.%M.%S')\n",
    "    timedelta = df['time'] - pd.datetime(df['time'].dt.year[0],df['time'].dt.month[0],df['time'].dt.day[0],0,0,0)\n",
    "    df['30_min'] = timedelta.dt.seconds/1800\n",
    "    df['30_min'] = df['30_min'].astype(int)\n",
    "    df['20_min'] = timedelta.dt.seconds/1200\n",
    "    df['20_min'] = df['20_min'].astype(int)\n",
    "    df['10_min'] = timedelta.dt.seconds/600\n",
    "    df['10_min'] = df['10_min'].astype(int)\n",
    "    df['5_min'] = timedelta.dt.seconds/300\n",
    "    df['5_min'] = df['5_min'].astype(int)\n",
    "\n",
    "    df['hour'] = df.START_DATE.apply(lambda x:int(x[11:13]))\n",
    "    \n",
    "    \n",
    "    df.TRADE_TYPE[df.TRADE_TYPE==21] = 1\n",
    "    df.TRADE_TYPE[df.TRADE_TYPE==22] = 0\n",
    "    \n",
    "    result = df.groupby(['date','TRADE_ADDRESS','INDUSTRY_CODE','CARD_TYPE_EX','CARD_TYPE','SAM_ID','TERMINAL_ID','hour','30_min','20_min','10_min','5_min']).TRADE_TYPE.agg(['count','sum']).reset_index()\n",
    "    result['inNums'] = result['sum']\n",
    "    result['outNums'] = result['count'] - result['sum']\n",
    "    del result['count'],result['sum']\n",
    "    \n",
    "    result.date = pd.to_datetime(result.date)\n",
    "    result['week'] = result.date.dt.dayofweek+1\n",
    "    del df\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_data(df):\n",
    "    sam_id_list = list(df.SAM_ID.unique())\n",
    "    for i in range(len(list(df.SAM_ID.unique()))):\n",
    "        df.SAM_ID[df.SAM_ID==sam_id_list[i]] = i\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_data2(df):\n",
    "    term_id_list = list(df.TERMINAL_ID.unique())\n",
    "    for i in range(len(list(df.TERMINAL_ID.unique()))):\n",
    "        df.TERMINAL_ID[df.TERMINAL_ID==term_id_list[i]] = i\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#根据EDA的分析进行数据的读取\n",
    "data_list = ['6','7','11','12','13','15','16']\n",
    "data1 = pd.DataFrame()\n",
    "for i in data_list:\n",
    "    file = str(i)+'_week.csv'\n",
    "    df = pd.read_csv(path+file)\n",
    "    df = pro_data(df)\n",
    "    df = pro_data2(df)\n",
    "    df = get_base_feature(df)\n",
    "    data1 = pd.concat([data1,df],axis=0,ignore_index=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进一步提取特征\n",
    "def more_feature(result):\n",
    "    tmp = result.copy()\n",
    "    ###按week计算每个站口客流量特征\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_ID_w_max'    : 'max',\n",
    "                                                                        'inNums_ID_w_min'    : 'min', \n",
    "                                                                        'inNums_ID_w_mean'   : 'mean',\n",
    "                                                                        'inNums_ID_w_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp, on=['TRADE_ADDRESS','week'], how='left')\n",
    "    ####按照week计算每个站口每小时的进站人数\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week','hour','30_min'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_ID_w_h_3_max'    : 'max',\n",
    "                                                                        'inNums_ID_w_h_3_min'    : 'min', \n",
    "                                                                        'inNums_ID_w_h_3_mean'   : 'mean',\n",
    "                                                                        'inNums_ID_w_h_3_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp,on=['TRADE_ADDRESS','week','hour','30_min'],how = 'left')\n",
    "    #20\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week','hour','20_min'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_ID_w_h_2_max'    : 'max',\n",
    "                                                                        'inNums_ID_w_h_2_min'    : 'min', \n",
    "                                                                        'inNums_ID_w_h_2_mean'   : 'mean',\n",
    "                                                                        'inNums_ID_w_h_2_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp,on=['TRADE_ADDRESS','week','hour','20_min'],how = 'left')\n",
    "    ###按照week计算每个站口每30分钟的进站人数\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week','hour'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_ID_w_h_max'    : 'max',\n",
    "                                                                        'inNums_ID_w_h_min'    : 'min', \n",
    "                                                                        'inNums_ID_w_h_mean'   : 'mean',\n",
    "                                                                        'inNums_ID_w_h_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp,on=['TRADE_ADDRESS','week','hour'],how = 'left')\n",
    "    \n",
    "    ###每个站口一周客流量特征\n",
    "    tmp = result.groupby(['TRADE_ADDRESS'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_ID_max'    : 'max',\n",
    "                                                                        'inNums_ID_min'    : 'min',\n",
    "                                                                        'inNums_ID_mean'   : 'mean',\n",
    "                                                                        'inNums_ID_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp, on=['TRADE_ADDRESS'], how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###出站与进站类似\n",
    "    \n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week'], as_index=False)['outNums'].agg({\n",
    "                                                                        'outNums_ID_w_max'    : 'max',\n",
    "                                                                        'outNums_ID_w_min'    : 'min',\n",
    "                                                                        'outNums_ID_w_mean'   : 'mean',\n",
    "                                                                        'outNums_ID_w_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp, on=['TRADE_ADDRESS','week'], how='left')\n",
    "    \n",
    "    \n",
    "    tmp = result.groupby(['TRADE_ADDRESS'], as_index=False)['outNums'].agg({\n",
    "                                                                        'outNums_ID_max'    : 'max',\n",
    "                                                                        'outNums_ID_min'    : 'min',\n",
    "                                                                        'outNums_ID_mean'   : 'mean',\n",
    "                                                                        'outNums_ID_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp, on=['TRADE_ADDRESS'], how='left')\n",
    "    ####按照week计算每个站口每小时的出站人数\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week','hour'], as_index=False)['outNums'].agg({\n",
    "                                                                        'outNums_ID_w_h_max'    : 'max',\n",
    "                                                                        'outNums_ID_w_h_min'    : 'min', \n",
    "                                                                        'outNums_ID_w_h_mean'   : 'mean',\n",
    "                                                                        'outNums_ID_w_h_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp,on=['TRADE_ADDRESS','week','hour'],how = 'left')\n",
    "    ####按照week计算每个站口每小时的进站人数\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week','hour','30_min'], as_index=False)['outNums'].agg({\n",
    "                                                                        'outNums_ID_w_h_3_max'    : 'max',\n",
    "                                                                        'outNums_ID_w_h_3_min'    : 'min', \n",
    "                                                                        'outNums_ID_w_h_3_mean'   : 'mean',\n",
    "                                                                        'outNums_ID_w_h_3_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp,on=['TRADE_ADDRESS','week','hour','30_min'],how = 'left')\n",
    "    #20\n",
    "    tmp = result.groupby(['TRADE_ADDRESS','week','hour','20_min'], as_index=False)['outNums'].agg({\n",
    "                                                                        'outNums_ID_w_h_2_max'    : 'max',\n",
    "                                                                        'outNums_ID_w_h_2_min'    : 'min', \n",
    "                                                                        'outNums_ID_w_h_2_mean'   : 'mean',\n",
    "                                                                        'outNums_ID_w_h_2_sum'   : 'sum'\n",
    "                                                                        })\n",
    "    result = result.merge(tmp,on=['TRADE_ADDRESS','week','hour','20_min'],how = 'left')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = more_feature(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5198855, 55)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inNums_ID_w_min 1.0\n",
      "inNums_ID_w_h_3_min 0.999800340651932\n",
      "inNums_ID_w_h_2_min 0.9998311166593413\n",
      "inNums_ID_w_h_min 0.9999717245431927\n",
      "inNums_ID_min 1.0\n",
      "outNums_ID_w_min 1.0\n",
      "outNums_ID_min 1.0\n",
      "outNums_ID_w_h_min 0.9994108318081577\n",
      "outNums_ID_w_h_3_min 0.9993708229985256\n",
      "outNums_ID_w_h_2_min 0.9993600513959324\n"
     ]
    }
   ],
   "source": [
    "#删除类别列别超过90%的列\n",
    "cols = list(data.columns)\n",
    "for col in cols:\n",
    "    rate = data[col].value_counts(normalize = True,dropna = False).values[0]\n",
    "    if rate>0.9:\n",
    "        cols.remove(col)\n",
    "        print(col,rate)\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id_week'] = 0   \n",
    "data.id_week[(data.date>='2015-9-8')&(data.date<='2015-9-14')] = 1\n",
    "data.id_week[(data.date>='2015-9-15')&(data.date<='2015-9-21')] = 2\n",
    "data.id_week[(data.date>='2015-10-13')&(data.date<='2015-10-19')] = 3\n",
    "data.id_week[(data.date>='2015-10-20')&(data.date<='2015-10-26')] = 4\n",
    "data.id_week[(data.date>='2015-10-27')&(data.date<='2015-11-2')] = 5\n",
    "data.id_week[(data.date>='2015-11-10')&(data.date<='2015-11-16')] = 6\n",
    "data.id_week[(data.date>='2015-11-17')&(data.date<='2015-11-23')] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = data[data.id_week==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[data.id_week==7]\n",
    "del target['id_week'],target['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################inNums验证集 4\n",
      "[0]\ttrain-mae:2.54464\tval-mae:2.59024\n",
      "Multiple eval metrics have been passed: 'val-mae' will be used for early stopping.\n",
      "\n",
      "Will train until val-mae hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mae:1.93373\tval-mae:2.56233\n",
      "Stopping. Best iteration:\n",
      "[69]\ttrain-mae:2.09217\tval-mae:2.55244\n",
      "\n",
      "验证集： 4\n",
      "验证集下一天作为测试集的误差为： 2.5298776626586914\n",
      "###############################inNums验证集 5\n",
      "[0]\ttrain-mae:2.55415\tval-mae:2.61962\n",
      "Multiple eval metrics have been passed: 'val-mae' will be used for early stopping.\n",
      "\n",
      "Will train until val-mae hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mae:1.91893\tval-mae:2.51824\n",
      "Stopping. Best iteration:\n",
      "[96]\ttrain-mae:1.93824\tval-mae:2.51715\n",
      "\n",
      "验证集： 5\n",
      "验证集下一天作为测试集的误差为： 2.4379775524139404\n",
      "###############################inNums验证集 6\n",
      "[0]\ttrain-mae:2.56691\tval-mae:2.58155\n",
      "Multiple eval metrics have been passed: 'val-mae' will be used for early stopping.\n",
      "\n",
      "Will train until val-mae hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mae:1.99347\tval-mae:2.49166\n",
      "[200]\ttrain-mae:1.66261\tval-mae:2.5274\n",
      "Stopping. Best iteration:\n",
      "[116]\ttrain-mae:1.9246\tval-mae:2.48942\n",
      "\n",
      "验证集： 6\n",
      "验证集下一天作为测试集的误差为： 2.2753026485443115\n",
      "inNums的CV验证分数： 2.414385954538981\n"
     ]
    }
   ],
   "source": [
    "#进行时间序列的交叉验证\n",
    "test_week = [4,5,6]\n",
    "# test_day = [23,24,25,28]\n",
    "error_in = []\n",
    "for i in test_week:\n",
    "    test = data[data.id_week== i+1]\n",
    "    y_test = test['inNums']\n",
    "    del test['id_week']\n",
    "    del test['inNums']\n",
    "    del test['outNums']\n",
    "    del test['date']\n",
    "    \n",
    "    print('###############################inNums验证集',i)\n",
    "\n",
    "    train = data[data.id_week<i]\n",
    "    valid = data[data.id_week==i]\n",
    "    y_train = train['inNums']\n",
    "    y_valid = valid['inNums']\n",
    "    \n",
    "    del train['inNums'],valid['inNums'],train['id_week'],train['date']\n",
    "    del train['outNums'],valid['outNums'],valid['id_week'],valid['date']\n",
    "\n",
    "    \n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.linear_model import LassoCV, RidgeCV\n",
    "   \n",
    "    dtrain = xgb.DMatrix(train, label = y_train)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "    dval = xgb.DMatrix(valid, label = y_valid)\n",
    "    watchlist = [(dtrain, 'train'),(dval, 'val')]\n",
    "    xgb_params = {'eta': 0.004, 'max_depth': 14, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'nthread': 4}\n",
    "    clf = xgb.train(dtrain=dtrain, num_boost_round=15000, evals=watchlist, early_stopping_rounds=100, verbose_eval=100, params=xgb_params)      \n",
    "    prediction_in = clf.predict(dtest, ntree_limit=clf.best_iteration)\n",
    "\n",
    "    error = mean_absolute_percentage_error(np.abs(np.round(prediction_in)),y_test)\n",
    "    error_in.append(error)\n",
    "    print('验证集：',i)\n",
    "    print('验证集下一天作为测试集的误差为：',error)\n",
    "        \n",
    "print('inNums的CV验证分数：',np.mean(error_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########inNums\n",
    "all_data = data.copy()\n",
    "\n",
    "del all_data['id_week'],all_data['date']\n",
    "\n",
    "train_in  = all_data\n",
    "y_train_in = all_data['inNums']\n",
    "del train_in['inNums'],train_in['outNums']\n",
    "\n",
    "test_in = target.copy()\n",
    "y_test_in = test_in['inNums']\n",
    "del test_in['inNums'],test_in['outNums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:2.56557\n",
      "Will train until train-mae hasn't improved in 100 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-35a39d9b9c6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprediction_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1109\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### all_data\n",
    "dtrain = xgb.DMatrix(train_in, label = y_train_in)\n",
    "dtest = xgb.DMatrix(test_in)\n",
    "watchlist = [(dtrain, 'train')]\n",
    "\n",
    "clf = xgb.train(dtrain=dtrain, num_boost_round=clf.best_iteration, early_stopping_rounds=100, evals=watchlist, verbose_eval=100, params=xgb_params)\n",
    "\n",
    "prediction_in = clf.predict(dtest, ntree_limit=clf.best_iteration)\n",
    "prediction = pd.DataFrame()\n",
    "prediction['inNums'] = prediction_in\n",
    "\n",
    "prediction['inNums'] = abs(np.round(prediction['inNums']))\n",
    "error_test_in = mean_absolute_percentage_error(abs(np.round(prediction['inNums'])),y_test)\n",
    "print('inNums的分数',error_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['pre_inNums'] = prediction_in\n",
    "#结果修正\n",
    "sub.loc[sub.pre_inNums<0,'pre_inNums'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行时间序列的交叉验证\n",
    "test_week = [4,5,6]\n",
    "# test_day = [23,24,25,28]\n",
    "error_in = []\n",
    "for i in test_week:\n",
    "    test = data[data.id_week== i+1]\n",
    "    y_test = test['outNums']\n",
    "    del test['id_week']\n",
    "    del test['inNums']\n",
    "    del test['outNums']\n",
    "    del test['date']\n",
    "    \n",
    "    print('###############################outNums验证集',i)\n",
    "\n",
    "    train = data[data.id_week<i]\n",
    "    valid = data[data.id_week==i]\n",
    "    y_train = train['outNums']\n",
    "    y_valid = valid['outNums']\n",
    "    \n",
    "    del train['inNums'],valid['inNums'],train['id_week'],train['date']\n",
    "    del train['outNums'],valid['outNums'],valid['id_week'],valid['date']\n",
    "\n",
    "    \n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.linear_model import LassoCV, RidgeCV\n",
    "   \n",
    "    dtrain = xgb.DMatrix(train, label = y_train)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "    dval = xgb.DMatrix(valid, label = y_valid)\n",
    "    watchlist = [(dtrain, 'train'),(dval, 'val')]\n",
    "    xgb_params = {'eta': 0.004, 'max_depth': 14, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'nthread': 4}\n",
    "    clf = xgb.train(dtrain=dtrain, num_boost_round=15000, evals=watchlist, early_stopping_rounds=100, verbose_eval=100, params=xgb_params)      \n",
    "    prediction_in = clf.predict(dtest, ntree_limit=clf.best_iteration)\n",
    "\n",
    "    error = mean_absolute_percentage_error(np.abs(np.round(prediction_in)),y_test)\n",
    "    error_in.append(error)\n",
    "    print('验证集：',i)\n",
    "    print('验证集下一天作为测试集的误差为：',error)\n",
    "        \n",
    "print('outNums的CV验证分数：',np.mean(error_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########outNums\n",
    "all_data = data.copy()\n",
    "del all_data['id_week'],all_data['date']\n",
    "\n",
    "train_out  = all_data\n",
    "y_train_out = all_data['outNums']\n",
    "del train_out['outNums'],train_out['inNums']\n",
    "\n",
    "test_out = target.copy()\n",
    "y_test_out = test_out['outNums']\n",
    "del test_out['inNums'],test_out['outNums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all_data\n",
    "dtrain = xgb.DMatrix(train_in, label = y_train_in)\n",
    "dtest = xgb.DMatrix(test_in)\n",
    "watchlist = [(dtrain, 'train')]\n",
    "\n",
    "clf = xgb.train(dtrain=dtrain, num_boost_round=clf.best_iteration, early_stopping_rounds=100, evals=watchlist, verbose_eval=100, params=xgb_params)\n",
    "\n",
    "prediction_in = clf.predict(dtest, ntree_limit=clf.best_iteration)\n",
    "prediction = pd.DataFrame()\n",
    "prediction['outNums'] = prediction_in\n",
    "\n",
    "prediction['outNums'] = abs(np.round(prediction['outNums']))\n",
    "error_test_in = mean_absolute_percentage_error(abs(np.round(prediction['outNums'])),y_test)\n",
    "print('outNums的分数',error_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['pre_outNums'] = prediction_in\n",
    "#结果修正\n",
    "sub.loc[sub.pre_outNums<0,'pre_outNums'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.groupby(['date','TRADE_ADDRESS']).pre_inNums.agg({'inNums':'sum'}).reset_index()\n",
    "sub2 = sub.groupby(['date','TRADE_ADDRESS']).pre_outNums.agg({'outNums':'sum'}).reset_index()\n",
    "sub_result = sub1.merge(sub2,on = ['date','TRADE_ADDRESS'],how='left')\n",
    "sub_result['flow'] = sub_result.inNums + sub_result.outNums\n",
    "sub_result['round_pre'] = np.round(sub_result.flow)\n",
    "sub_result['round_in'] = np.round(sub_result.inNums)\n",
    "sub_result['round_out'] = np.round(sub_result.outNums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real1 = sub.groupby(['date','TRADE_ADDRESS']).inNums.agg({'inNums':'sum'}).reset_index()\n",
    "real2 = sub.groupby(['date','TRADE_ADDRESS']).outNums.agg({'outNums':'sum'}).reset_index()\n",
    "real = real1.merge(real2,on = ['date','TRADE_ADDRESS'],how='left')\n",
    "real['flow'] = real.inNums + real.outNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_absolute_percentage_error(sub_result.inNums,real.inNums)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#log\n",
    "inNums的CV验证分数： 2.531604448954264\n",
    "outNums的CV验证分数： 1.5056676864624023\n",
    "########\n",
    "inNums的CV验证分数：2.414385954538981\n",
    "outNums的CV验证分数： 1.4521469275156658"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
